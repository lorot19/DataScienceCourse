{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a576b1f1-459f-405d-b625-9d9455234c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/tomaslorincfpt/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/tomaslorincfpt/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/tomaslorincfpt/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import sys\n",
    "import re\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pickle\n",
    "\n",
    "nltk.download(['punkt', 'wordnet', 'omw-1.4'])\n",
    "\n",
    "# define regex characters\n",
    "url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9efbf60-d0ef-4597-8be7-6726f4572a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Function which tokenize message using regular expressions\n",
    "    :param text: String containing message\n",
    "    :return: clean_tokens: list of words containing tokenized and cleaned message\n",
    "    \"\"\"\n",
    "    # get list of all urls using regex\n",
    "    detected_urls = re.findall(url_regex, text)\n",
    "    # replace each url in text string with placeholder\n",
    "    for url in detected_urls:\n",
    "        text = text.replace(url, \"urlplaceholder\")\n",
    "    # tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # iterate through each token\n",
    "    clean_tokens = []\n",
    "    for tok in tokens:\n",
    "        # lemmatize, normalize case, and remove leading/trailing white space\n",
    "        clean_tok = lemmatizer.lemmatize(tok).lower().strip()\n",
    "        clean_tokens.append(clean_tok)\n",
    "    return clean_tokens\n",
    "\n",
    "def display_results(cv, y_test, y_pred, labels):\n",
    "    \"\"\"\n",
    "    This function visualise trained model\n",
    "    :param cv: Any\n",
    "    :param y_test: Any\n",
    "    :param y_pred: Any\n",
    "    :param labels: Any\n",
    "    :return: none\n",
    "    \"\"\"\n",
    "    accuracy = (y_pred == y_test).mean()\n",
    "    print(\"Labels:\", labels)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"\\nBest Parameters:\", cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3c51093-d650-4989-aab4-cefca004fdc4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Table name not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# load data\u001b[39;00m\n\u001b[1;32m      2\u001b[0m engine \u001b[38;5;241m=\u001b[39m create_engine(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msqlite:///moviesdata.db\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_sql_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m X \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m Y \u001b[38;5;241m=\u001b[39m df[df\u001b[38;5;241m.\u001b[39mcolumns]\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m'\u001b[39m], axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/pandas/io/sql.py:286\u001b[0m, in \u001b[0;36mread_sql_table\u001b[0;34m(table_name, con, schema, index_col, coerce_float, parse_dates, columns, chunksize)\u001b[0m\n\u001b[1;32m    284\u001b[0m pandas_sql \u001b[38;5;241m=\u001b[39m pandasSQL_builder(con, schema\u001b[38;5;241m=\u001b[39mschema)\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pandas_sql\u001b[38;5;241m.\u001b[39mhas_table(table_name):\n\u001b[0;32m--> 286\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTable \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    288\u001b[0m table \u001b[38;5;241m=\u001b[39m pandas_sql\u001b[38;5;241m.\u001b[39mread_table(\n\u001b[1;32m    289\u001b[0m     table_name,\n\u001b[1;32m    290\u001b[0m     index_col\u001b[38;5;241m=\u001b[39mindex_col,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    294\u001b[0m     chunksize\u001b[38;5;241m=\u001b[39mchunksize,\n\u001b[1;32m    295\u001b[0m )\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m table \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Table name not found"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "engine = create_engine('sqlite:///moviesdata.db')\n",
    "df = pd.read_sql_table(table_name='name', con=engine)\n",
    "X = df['description']\n",
    "Y = df[df.columns].drop(['description'], axis = 1)\n",
    "\n",
    "Y = Y.astype(int)\n",
    "categories = Y.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90810c7-94e1-4ca1-9fe8-197454139485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7ca764-7174-4d04-bc3b-90afd04184cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "    ('Tfidf', TfidfTransformer()),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier()))])\n",
    "parameters = {'clf__estimator__n_estimators': [3]}\n",
    "\n",
    "model = GridSearchCV(pipeline, param_grid=parameters, verbose=3, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68606834-c4e9-42e3-bde3-96d7d1e89ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83dea50-0f87-4a51-8135-3e696f3c12cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display results\n",
    "y_pred = model.predict(X_test)\n",
    "display_results(model, Y_test, y_pred, categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49856b78-6781-43ce-a7c1-1beaf13ff3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "with open('netflix_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f947645a-def3-4bdc-b786-2da4080261b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
