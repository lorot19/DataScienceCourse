{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b84015f-f771-492d-bb1a-1465afeecd7e",
   "metadata": {},
   "source": [
    "### Capston project - Netflix movies - model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a576b1f1-459f-405d-b625-9d9455234c16",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3568864471.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [1]\u001b[0;36m\u001b[0m\n\u001b[0;31m    from nltk.stem aimport WordNetLemmatizer\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import sys\n",
    "import re\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pickle\n",
    "\n",
    "nltk.download(['punkt', 'wordnet', 'omw-1.4'])\n",
    "\n",
    "# define regex characters\n",
    "url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9efbf60-d0ef-4597-8be7-6726f4572a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define tokenize function used for text transformation\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Function which tokenize message using regular expressions\n",
    "    :param text: String containing message\n",
    "    :return: clean_tokens: list of words containing tokenized and cleaned message\n",
    "    \"\"\"\n",
    "    # get list of all urls using regex\n",
    "    detected_urls = re.findall(url_regex, text)\n",
    "    # replace each url in text string with placeholder\n",
    "    for url in detected_urls:\n",
    "        text = text.replace(url, \"urlplaceholder\")\n",
    "    # tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # iterate through each token\n",
    "    clean_tokens = []\n",
    "    for tok in tokens:\n",
    "        # lemmatize, normalize case, and remove leading/trailing white space\n",
    "        clean_tok = lemmatizer.lemmatize(tok).lower().strip()\n",
    "        clean_tokens.append(clean_tok)\n",
    "    return clean_tokens\n",
    "\n",
    "# define function which displays results of model training\n",
    "def display_results(cv, y_test, y_pred, labels):\n",
    "    \"\"\"\n",
    "    This function visualise trained model\n",
    "    :param cv: Any\n",
    "    :param y_test: Any\n",
    "    :param y_pred: Any\n",
    "    :param labels: Any\n",
    "    :return: none\n",
    "    \"\"\"\n",
    "    accuracy = (y_pred == y_test).mean()\n",
    "    print(\"Labels:\", labels)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"\\nBest Parameters:\", cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5562f9-80d2-43b0-82b8-fbee233cfbbd",
   "metadata": {},
   "source": [
    "### First we need to load data from SQL DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c51093-d650-4989-aab4-cefca004fdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "engine = create_engine('sqlite:///app/moviesdata.db')\n",
    "df = pd.read_sql_table(table_name='name', con=engine)\n",
    "X = df['title']\n",
    "Y = df[df.columns].drop(['title'], axis = 1)\n",
    "\n",
    "Y = Y.astype(int)\n",
    "categories = Y.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb370721-85ae-4624-a253-0c39fa6d229f",
   "metadata": {},
   "source": [
    "### Then we need to split them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90810c7-94e1-4ca1-9fe8-197454139485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62373f09-6edd-4820-90fa-4cda94eb02e8",
   "metadata": {},
   "source": [
    "### and create model using pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7ca764-7174-4d04-bc3b-90afd04184cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "    ('Tfidf', TfidfTransformer()),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier()))])\n",
    "parameters = {'clf__estimator__n_estimators': [50]}\n",
    "\n",
    "model = GridSearchCV(pipeline, param_grid=parameters, verbose=3, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33071a5-e0b7-4346-9f5d-3a0e15fa1bcf",
   "metadata": {},
   "source": [
    "### Now we are able to fit our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68606834-c4e9-42e3-bde3-96d7d1e89ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136752c1-d360-44fd-8f4a-a26eb581968f",
   "metadata": {},
   "source": [
    "### Analyse results\n",
    "\n",
    "as we can see our results are prety good. We have nice accuracy in all geners expect drama which could be caused just because almost every movie has little bit of drama which makes this category less accurate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83dea50-0f87-4a51-8135-3e696f3c12cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display results\n",
    "y_pred = model.predict(X_test)\n",
    "display_results(model, Y_test, y_pred, categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4c646b-b7f8-41b5-be63-2f4c1e916a19",
   "metadata": {},
   "source": [
    "### Final step - save model into pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49856b78-6781-43ce-a7c1-1beaf13ff3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "with open('app/netflix_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80a6069-835f-4665-961d-b69d136c9763",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
